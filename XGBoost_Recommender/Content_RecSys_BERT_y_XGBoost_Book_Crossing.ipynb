{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNI7h9ZWQ6t5ouM2fv/0dNf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JCaballerot/Recommender-Systems/blob/main/XGBoost_Recommender/Content_RecSys_BERT_y_XGBoost_Book_Crossing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  <img src=\"https://storage.googleapis.com/kaggle-datasets-images/1661575/2726067/684ac0c4c14cb46d1047ccb620b45cac/dataset-cover.jpg?t=2021-10-21-03-18-09\" width=\"800\" height=\"300\">\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "2yFbptc-h4bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Content RecSys: BERT y XGBoost Book-Crossing**"
      ],
      "metadata": {
        "id": "6j7nVu4kg-RF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "## Tabla de Contenidos\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>\n",
        "    \n",
        "1. <a href=\"#item1\">Introducción</a>  \n",
        "2. <a href=\"#item4\">Descripción del Dataset</a>  \n",
        "3. <a href=\"#item4\">Preprocesamiento de Datos</a>  \n",
        "4. <a href=\"#item4\">Muestreo/Enmascaramiento</a>  \n",
        "5. <a href=\"#item4\">Feature engineering</a>  \n",
        "6. <a href=\"#item4\">Preparación de los Datos para el Modelo</a>  \n",
        "7. <a href=\"#item4\">Entrenamiento del Modelo con XGBoost</a>  \n",
        "8. <a href=\"#item4\">Evaluación del Modelo</a>  \n",
        "9. <a href=\"#item4\">Generación de Recomendaciones</a>  \n",
        "10. <a href=\"#item4\">Conclusiones</a>  \n",
        "\n",
        "</font>\n",
        "</div>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aUw1RvzfhQ1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introducción\n",
        "\n",
        "El dataset de **Book-Crossing** es un recurso rico en información sobre las interacciones de usuarios con libros, capturando calificaciones, títulos, autores y otros datos relevantes. Este laboratorio propone desarrollar un sistema de recomendación basado en **machine learning**, combinando técnicas avanzadas de procesamiento de texto con **BERT** y aprendizaje supervisado mediante **XGBoost**.\n",
        "\n",
        "El objetivo principal es predecir si un usuario disfrutará un libro en particular, basado en sus interacciones pasadas y características tanto del usuario como del libro. Además, se busca generar recomendaciones personalizadas y evaluar métricas como la diversidad global de las recomendaciones.\n",
        "\n",
        "A lo largo del laboratorio, se explorarán diferentes técnicas de ingeniería de características, como el uso de variables cuantitativas, target encoding para variables categóricas, y **embeddings** textuales obtenidos con **BERT**. Finalmente, se entrenará un modelo de **XGBoost**, y se evaluará su desempeño utilizando métricas como el coeficiente de Gini."
      ],
      "metadata": {
        "id": "Rd6n5DqThtkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Descripción del Dataset\n",
        "\n",
        "El dataset de Book-Crossing contiene 3 archivos principales:\n",
        "\n",
        "- **BX-Users.csv:** Información sobre los usuarios.\n",
        "- **BX-Books.csv:** Información sobre los libros.\n",
        "- **BX-Book-Ratings.csv:** Calificaciones dadas por los usuarios a los libros.\n",
        "\n",
        "El dataset puede ser descargado desde: Book-Crossing Dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "h0B39u5LjbSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"3.1\"></a>\n",
        "\n",
        "**2.1. BX-Users.csv**\n",
        "\n",
        "Contiene información de los usuarios:\n",
        "\n",
        "- **User-ID:** Identificador único del usuario.\n",
        "- **Location:** Ubicación del usuario.\n",
        "- **Age:** Edad del usuario.\n",
        "\n"
      ],
      "metadata": {
        "id": "viNozbdwj5sa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"3.2\"></a>\n",
        "\n",
        "**2.2. BX-Books.csv**\n",
        "\n",
        "Contiene información de los libros:\n",
        "\n",
        "- **SBN:** Identificador único del libro.\n",
        "- **Book-Title:** Título del libro.\n",
        "- **Book-Author:** Autor del libro.\n",
        "- **Year-Of-Publication:** Año de publicación.\n",
        "- **Publisher:** Editorial.\n"
      ],
      "metadata": {
        "id": "iXwqVRGOj7V0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"3.3\"></a>\n",
        "\n",
        "**2.3. BX-Book-Ratings.csv**\n",
        "Contiene las calificaciones de los usuarios:\n",
        "\n",
        "- **User-ID:** Identificador del usuario.\n",
        "- **ISBN:** Identificador del libro.\n",
        "- **Book-Rating:** Calificación dada al libro (0-10).\n"
      ],
      "metadata": {
        "id": "xU5j7ggbj8xv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preprocesamiento de Datos\n"
      ],
      "metadata": {
        "id": "Hg8hForFkfBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección, cargaremos los datos y realizaremos una limpieza y exploración inicial.\n",
        "\n"
      ],
      "metadata": {
        "id": "IOcS1BH7khhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1. Carga de los Datos**"
      ],
      "metadata": {
        "id": "XW_B8CXnkmkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Crear la carpeta llamada 'dataset'\n",
        "!mkdir -p dataset\n",
        "\n",
        "# Descargar los archivos CSV y guardarlos en la carpeta 'dataset'\n",
        "!wget -O dataset/BX-Book-Ratings.csv https://raw.githubusercontent.com/bigsnarfdude/guide-to-data-mining/master/BX-Dump/BX-Book-Ratings.csv\n",
        "!wget -O dataset/BX-Books.csv https://raw.githubusercontent.com/bigsnarfdude/guide-to-data-mining/master/BX-Dump/BX-Books.csv\n",
        "!wget -O dataset/BX-Users.csv https://raw.githubusercontent.com/bigsnarfdude/guide-to-data-mining/master/BX-Dump/BX-Users.csv\n"
      ],
      "metadata": {
        "id": "-pyiY5E-W5gs",
        "outputId": "e571ed59-f099-4bc1-b8f6-47962ce010f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-30 22:37:27--  https://raw.githubusercontent.com/bigsnarfdude/guide-to-data-mining/master/BX-Dump/BX-Book-Ratings.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29532460 (28M) [text/plain]\n",
            "Saving to: ‘dataset/BX-Book-Ratings.csv’\n",
            "\n",
            "dataset/BX-Book-Rat 100%[===================>]  28.16M  44.2MB/s    in 0.6s    \n",
            "\n",
            "2024-11-30 22:37:28 (44.2 MB/s) - ‘dataset/BX-Book-Ratings.csv’ saved [29532460/29532460]\n",
            "\n",
            "--2024-11-30 22:37:28--  https://raw.githubusercontent.com/bigsnarfdude/guide-to-data-mining/master/BX-Dump/BX-Books.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77515949 (74M) [text/plain]\n",
            "Saving to: ‘dataset/BX-Books.csv’\n",
            "\n",
            "dataset/BX-Books.cs 100%[===================>]  73.92M  59.0MB/s    in 1.3s    \n",
            "\n",
            "2024-11-30 22:37:30 (59.0 MB/s) - ‘dataset/BX-Books.csv’ saved [77515949/77515949]\n",
            "\n",
            "--2024-11-30 22:37:30--  https://raw.githubusercontent.com/bigsnarfdude/guide-to-data-mining/master/BX-Dump/BX-Users.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11783733 (11M) [text/plain]\n",
            "Saving to: ‘dataset/BX-Users.csv’\n",
            "\n",
            "dataset/BX-Users.cs 100%[===================>]  11.24M  25.0MB/s    in 0.4s    \n",
            "\n",
            "2024-11-30 22:37:31 (25.0 MB/s) - ‘dataset/BX-Users.csv’ saved [11783733/11783733]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las librerías necesarias\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar los archivos CSV\n",
        "ratings = pd.read_csv(\"dataset/BX-Book-Ratings.csv\", sep=\";\", encoding=\"ISO-8859-1\")\n",
        "books = pd.read_csv(\"dataset/BX-Books.csv\", sep=\";\", encoding=\"ISO-8859-1\", on_bad_lines=\"skip\", low_memory=False)\n",
        "users = pd.read_csv(\"dataset/BX-Users.csv\", sep=\";\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# Asignar manualmente los nombres de las columnas\n",
        "ratings.columns = ['User-ID', 'ISBN', 'Book-Rating']\n",
        "books.columns = ['ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L']\n",
        "users.columns = ['User-ID', 'Location', 'Age']\n"
      ],
      "metadata": {
        "id": "rAd2-KbXl1QN"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2. Exploración y Limpieza**\n",
        "\n",
        "Visualizar las primeras filas de cada dataframe:"
      ],
      "metadata": {
        "id": "3pjdAPKml3C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras filas de ratings\n",
        "print(ratings.head())\n",
        "\n",
        "# Mostrar las primeras filas de books\n",
        "print(books.head())\n",
        "\n",
        "# Mostrar las primeras filas de users\n",
        "print(users.head())\n"
      ],
      "metadata": {
        "id": "StsxWPI5kmC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limpieza de Datos**\n",
        "\n",
        "Conversión de tipos de datos.\n",
        "\n",
        "Manejo de valores faltantes."
      ],
      "metadata": {
        "id": "ibOhppYnl9J5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir 'Year-Of-Publication' a numérico y manejar errores\n",
        "books['Year-Of-Publication'] = pd.to_numeric(books['Year-Of-Publication'], errors='coerce')\n",
        "books['Year-Of-Publication'].fillna(books['Year-Of-Publication'].median(), inplace=True)\n",
        "books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(int)\n"
      ],
      "metadata": {
        "id": "kd9dOkTTmCaM",
        "outputId": "241e76ab-7570-45d0-9ef3-6be368ba14b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-203-7f58c4612ec2>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  books['Year-Of-Publication'].fillna(books['Year-Of-Publication'].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manejar valores faltantes en 'Book-Author' y 'Publisher'\n",
        "books['Book-Author'].fillna('Unknown', inplace=True)\n",
        "books['Publisher'].fillna('Unknown', inplace=True)\n"
      ],
      "metadata": {
        "id": "gs6G28-3mD3I",
        "outputId": "c6361a11-0b53-412b-a288-8ec06847e85f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-204-71b7ce4f9bdd>:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  books['Book-Author'].fillna('Unknown', inplace=True)\n",
            "<ipython-input-204-71b7ce4f9bdd>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  books['Publisher'].fillna('Unknown', inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir 'Age' a numérico y manejar errores\n",
        "users['Age'] = pd.to_numeric(users['Age'], errors='coerce')\n",
        "users['Age'].fillna(users['Age'].median(), inplace=True)\n",
        "users['Age'] = users['Age'].astype(int)"
      ],
      "metadata": {
        "id": "aoWPieojmE4D",
        "outputId": "aee3f927-ce6f-426c-f6ca-4540481f78e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-205-9482be963f6e>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  users['Age'].fillna(users['Age'].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. División de los Datos\n",
        "\n",
        "Para evaluar el modelo de manera adecuada, separaremos un 10% de las calificaciones de cada usuario para utilizarlas como conjunto de prueba."
      ],
      "metadata": {
        "id": "ndcDbvlZmIUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "# Unir ratings con usuarios y libros\n",
        "data = ratings.merge(users, on='User-ID').merge(books, on='ISBN')[:10000]\n",
        "\n",
        "# Binarizar el target: 1 si Book-Rating > 5, 0 si <= 5\n",
        "data['target'] = (data['Book-Rating'] > 7).astype(int)\n"
      ],
      "metadata": {
        "id": "s_ejJG2rmNo_"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar en entrenamiento y prueba por usuario\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
        "train_idx, test_idx = next(gss.split(data, groups=data['User-ID']))\n",
        "\n",
        "train_data = data.iloc[train_idx].reset_index(drop=True)\n",
        "test_data = data.iloc[test_idx].reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "D-QpGwkVmQ5Q"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Feature Engineering\n",
        "\n",
        "En esta sección, crearemos nuevas características que ayuden al modelo a predecir con mayor precisión."
      ],
      "metadata": {
        "id": "G3NTPuOCl6Wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.1. Variables Cuantitativas**\n",
        "\n",
        "- Edad del usuario (Age).\n",
        "\n",
        "- Antigüedad del libro: Año actual menos el año de publicación."
      ],
      "metadata": {
        "id": "PudN4w2_mZO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "current_year = datetime.now().year\n",
        "\n",
        "# Calcular la antigüedad del libro\n",
        "train_data['Book-Age'] = current_year - train_data['Year-Of-Publication']\n",
        "test_data['Book-Age'] = current_year - test_data['Year-Of-Publication']\n"
      ],
      "metadata": {
        "id": "M5EUYEg5mdOl"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.2. Target Encoding**\n",
        "\n",
        "Aplicaremos target encoding a las variables categóricas:\n",
        "\n",
        "- Location\n",
        "- Book-Author\n",
        "- Publisher"
      ],
      "metadata": {
        "id": "9aZoEE3Ymf64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip3 install category_encoders"
      ],
      "metadata": {
        "id": "9EPwMTWDXvRX"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from category_encoders import TargetEncoder\n",
        "\n",
        "categorical_features = ['Location', 'Book-Author', 'Publisher']\n",
        "\n",
        "# Crear el encoder para las variables categóricas\n",
        "encoder = TargetEncoder(cols=categorical_features)\n",
        "encoder.fit(train_data[categorical_features], train_data['target'])\n",
        "\n",
        "# Transformar las variables categóricas en los datos de entrenamiento y prueba\n",
        "train_encoded = encoder.transform(train_data[categorical_features])\n",
        "test_encoded = encoder.transform(test_data[categorical_features])\n",
        "\n",
        "# Cambiar el nombre de las columnas codificadas\n",
        "train_encoded.columns = [f\"target_coded_{col}\" for col in train_encoded.columns]\n",
        "test_encoded.columns = [f\"target_coded_{col}\" for col in test_encoded.columns]\n",
        "\n",
        "# Añadir las variables codificadas al dataframe original\n",
        "train_data = pd.concat([train_data, train_encoded], axis=1)\n",
        "test_data = pd.concat([test_data, test_encoded], axis=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KQT4pZlBmteK"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.3. Generación de Embeddings con BERT**\n",
        "\n",
        "Utilizaremos BERT para convertir el título del libro en embeddings numéricos que capturen su semántica.\n",
        "\n"
      ],
      "metadata": {
        "id": "Enz7_lwtmwOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instalación y Carga de BERT**"
      ],
      "metadata": {
        "id": "yMON78f6m26j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar la biblioteca transformers\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Cargar el tokenizador y el modelo preentrenado\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "id": "tyedF56km5Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función para Obtener Embeddings"
      ],
      "metadata": {
        "id": "eRKZBg1-m8dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embedding(text):\n",
        "    # Tokenización y codificación\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=20)\n",
        "    # Obtener las representaciones del modelo\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Usar el embedding del token [CLS] como representación\n",
        "    embedding = outputs.last_hidden_state[:,0,:].numpy()\n",
        "    return embedding.flatten()\n"
      ],
      "metadata": {
        "id": "QCKKjOixm_ZS"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aplicar la Función a los Datos**\n",
        "\n",
        "Debido a limitaciones computacionales, podemos trabajar con una muestra."
      ],
      "metadata": {
        "id": "pZDS75uInBuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%time\n",
        "#import numpy as np\n",
        "\n",
        "# Obtener embeddings para el título del libro en el conjunto de entrenamiento\n",
        "#train_embeddings = np.vstack(train_data['Book-Title'].apply(get_bert_embedding))\n",
        "\n",
        "# Obtener embeddings para el conjunto de prueba\n",
        "#test_embeddings = np.vstack(test_data['Book-Title'].apply(get_bert_embedding))\n"
      ],
      "metadata": {
        "id": "Dzkf8onInEZG"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convierte los embeddings en un DataFrame\n",
        "train_embeddings_df = pd.DataFrame(train_embeddings, columns=[f\"Title_embedding_{i}\" for i in range(train_embeddings.shape[1])])\n",
        "train_data = pd.concat([train_data.reset_index(drop=True), train_embeddings_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Convierte los embeddings en un DataFrame\n",
        "test_embeddings_df = pd.DataFrame(test_embeddings, columns=[f\"Title_embedding_{i}\" for i in range(test_embeddings.shape[1])])\n",
        "test_data = pd.concat([test_data.reset_index(drop=True), test_embeddings_df.reset_index(drop=True)], axis=1)\n"
      ],
      "metadata": {
        "id": "AFIsbM8C42cW"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "6wA4lC-F6G2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.4. One-Hot Encoding basado en Productos Calificados**\n",
        "\n",
        "Crearemos características que indiquen si el usuario ya ha calificado el libro y el puntaje que le dio previamente."
      ],
      "metadata": {
        "id": "L0IVVkgVnGcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Crear un Diccionario de Calificaciones por Usuario**"
      ],
      "metadata": {
        "id": "3pL_f7hVnl9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[['User-ID', 'ISBN', 'Book-Rating']]"
      ],
      "metadata": {
        "id": "9p2DsB3b6pXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar One-Hot Encoding sobre ISBN con los valores de 'Book-Rating'\n",
        "one_hot_encoded = data.pivot_table(\n",
        "    index='User-ID',\n",
        "    columns='ISBN',\n",
        "    values='Book-Rating')\n",
        "\n",
        "# Ajustar los nombres de las columnas para incluir el prefijo 'book-rating-'\n",
        "one_hot_encoded.columns = [f\"book-rating-{isbn}\" for isbn in one_hot_encoded.columns]\n",
        "one_hot_encoded.head()"
      ],
      "metadata": {
        "id": "u6snrPC88j1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aplicar la Función a los Datos**"
      ],
      "metadata": {
        "id": "fx--c5RQnqLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Asegúrate de que los índices sean consistentes\n",
        "one_hot_encoded = one_hot_encoded.reset_index()  # Reseteamos el índice para usar 'User-ID' como columna\n",
        "\n",
        "# Merge con train_data\n",
        "train_data_merged = train_data.merge(one_hot_encoded, on='User-ID', how='left')\n",
        "\n",
        "# Merge con test_data\n",
        "test_data_merged = test_data.merge(one_hot_encoded, on='User-ID', how='left')\n"
      ],
      "metadata": {
        "id": "JHBBkPkiZ313"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "limpiamos los registros que coinciden con el target"
      ],
      "metadata": {
        "id": "drYYXTqmBEO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para train_data\n",
        "for idx, isbn in zip(train_data_merged.index, train_data_merged['ISBN']):\n",
        "    column_to_nullify = f\"book-rating-{isbn}\"\n",
        "    if column_to_nullify in train_data_merged.columns:\n",
        "        train_data_merged.loc[idx, column_to_nullify] = np.nan\n",
        "\n",
        "# Para test_data\n",
        "for idx, isbn in zip(test_data_merged.index, test_data_merged['ISBN']):\n",
        "    column_to_nullify = f\"book-rating-{isbn}\"\n",
        "    if column_to_nullify in test_data_merged.columns:\n",
        "        test_data_merged.loc[idx, column_to_nullify] = np.nan\n"
      ],
      "metadata": {
        "id": "bHBcDyZC-4Fa"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Preparación de los Datos para el Modelo\n",
        "\n",
        "Seleccionaremos las características y prepararemos los conjuntos de entrenamiento y prueba."
      ],
      "metadata": {
        "id": "KfWBHP2-n5bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_merged.columns.tolist()"
      ],
      "metadata": {
        "id": "ACpIAhZsBhA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables cuantitativas\n",
        "features = ['Age', 'Book-Age'] + \\\n",
        "           [x for x in test_data_merged.columns if 'target_coded_' in x] +\\\n",
        "           [x for x in test_data_merged.columns if 'book-rating-' in x] +\\\n",
        "           [x for x in test_data_merged.columns if 'Title_embedding_' in x]\n",
        "\n",
        "features = list(set(features))"
      ],
      "metadata": {
        "id": "ZZBuBLMqaL9i"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combinar todas las características\n",
        "X_train = train_data_merged[features]\n",
        "X_test = test_data_merged[features]\n",
        "\n",
        "# Variables objetivo\n",
        "y_train = train_data_merged['target']\n",
        "y_test = test_data_merged['target']\n"
      ],
      "metadata": {
        "id": "-HB85yaNn9FE"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PyiPg2ENZxT",
        "outputId": "b4b79b44-5895-4345-dfee-d73e391ae850"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8842, 9852)"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb-vR6lhNb5x",
        "outputId": "85d40be4-19b7-4c20-9c48-b9d801ff49d6"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1158, 9852)"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Entrenamiento del Modelo con XGBoost\n",
        "\n",
        "Entrenaremos un modelo de clasificación utilizando XGBoost."
      ],
      "metadata": {
        "id": "bb8t1LX6n_EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar XGBoost\n",
        "!pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ggqhMroG7zC",
        "outputId": "fadf0f4e-60b3-4a49-c8d8-2c6c24ecd329"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Dividir un conjunto de validación\n",
        "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "X_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yV6nrrgNfts",
        "outputId": "a53aa031-f18d-4b55-c908-d6d16c332991"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1769, 9852)"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear las DMatrix para XGBoost\n",
        "dtrain = xgb.DMatrix(X_train_split.values, label=y_train_split, feature_names=features)\n",
        "dval   = xgb.DMatrix(X_val.values, label=y_val, feature_names=features)\n"
      ],
      "metadata": {
        "id": "gl16IuUgNjtl"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Configuración de los parámetros del modelo\n",
        "params = {\n",
        "    \"objective\": \"binary:logistic\",  # Cambiar si es un problema multiclase\n",
        "    \"max_depth\": 6,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"min_child_weight\": 200,\n",
        "    \"subsample\": 0.4,\n",
        "    \"colsample_bytree\": 0.4,\n",
        "    \"eval_metric\": \"auc\"\n",
        "}\n",
        "\n",
        "# Entrenar con early stopping\n",
        "evals = [(dtrain, \"train\"), (dval, \"eval\")]\n",
        "model = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=100,\n",
        "    evals=evals,\n",
        "    early_stopping_rounds=10,\n",
        "    verbose_eval=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "y9E2YG-poBzm",
        "outputId": "ae398308-718e-47a3-d3d9-977928807dae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-auc:0.76824\teval-auc:0.76362\n",
            "[1]\ttrain-auc:0.79987\teval-auc:0.79768\n",
            "[2]\ttrain-auc:0.79937\teval-auc:0.79687\n",
            "[3]\ttrain-auc:0.80616\teval-auc:0.80617\n",
            "[4]\ttrain-auc:0.80646\teval-auc:0.80655\n",
            "[5]\ttrain-auc:0.80623\teval-auc:0.80616\n",
            "[6]\ttrain-auc:0.80646\teval-auc:0.80655\n",
            "[7]\ttrain-auc:0.80643\teval-auc:0.80646\n",
            "[8]\ttrain-auc:0.80643\teval-auc:0.80645\n",
            "[9]\ttrain-auc:0.80643\teval-auc:0.80644\n",
            "[10]\ttrain-auc:0.80643\teval-auc:0.80644\n",
            "[11]\ttrain-auc:0.80625\teval-auc:0.80621\n",
            "[12]\ttrain-auc:0.80643\teval-auc:0.80644\n",
            "[13]\ttrain-auc:0.80642\teval-auc:0.80639\n",
            "[14]\ttrain-auc:0.85385\teval-auc:0.84967\n",
            "[15]\ttrain-auc:0.85386\teval-auc:0.84970\n",
            "[16]\ttrain-auc:0.85418\teval-auc:0.84963\n",
            "[17]\ttrain-auc:0.85506\teval-auc:0.85430\n",
            "[18]\ttrain-auc:0.85519\teval-auc:0.85445\n",
            "[19]\ttrain-auc:0.85715\teval-auc:0.85560\n",
            "[20]\ttrain-auc:0.85982\teval-auc:0.85710\n",
            "[21]\ttrain-auc:0.86004\teval-auc:0.85743\n",
            "[22]\ttrain-auc:0.86116\teval-auc:0.86355\n",
            "[23]\ttrain-auc:0.86116\teval-auc:0.86082\n",
            "[24]\ttrain-auc:0.86118\teval-auc:0.86095\n",
            "[25]\ttrain-auc:0.86120\teval-auc:0.86107\n",
            "[26]\ttrain-auc:0.86120\teval-auc:0.86103\n",
            "[27]\ttrain-auc:0.86119\teval-auc:0.86101\n",
            "[28]\ttrain-auc:0.86182\teval-auc:0.86189\n",
            "[29]\ttrain-auc:0.86224\teval-auc:0.86074\n",
            "[30]\ttrain-auc:0.86228\teval-auc:0.85867\n",
            "[31]\ttrain-auc:0.86227\teval-auc:0.85860\n",
            "[32]\ttrain-auc:0.86229\teval-auc:0.85723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsFM7jUCbrZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Evaluación del Modelo\n",
        "\n",
        "Evaluaremos el rendimiento del modelo utilizando el coeficiente de Gini."
      ],
      "metadata": {
        "id": "3HSy5FvPoFRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluar el Modelo"
      ],
      "metadata": {
        "id": "PKdiwThHoPDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Predecir probabilidades en el conjunto de prueba\n",
        "y_probs = model.predict(xgb.DMatrix(X_train_split))\n",
        "\n",
        "# Calcular el Gini utilizando AUC\n",
        "auc = roc_auc_score(y_train_split, y_probs)\n",
        "auc*2-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRJFTTc3KfLs",
        "outputId": "e1a79d6c-b264-406b-fdb4-f7f37370f9fe"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7245825448277459"
            ]
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Predecir probabilidades en el conjunto de prueba\n",
        "y_probs = model.predict(xgb.DMatrix(X_val))\n",
        "\n",
        "# Calcular el Gini utilizando AUC\n",
        "auc = roc_auc_score(y_val, y_probs)\n",
        "auc*2-1"
      ],
      "metadata": {
        "id": "RhAy2B6MoRQi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1851ea74-2e63-4703-e101-7b6aa0b4e4bb"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7144664750862608"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Generación de Recomendaciones\n",
        "\n",
        "Generaremos recomendaciones personalizadas para los usuarios y calcularemos la diversidad global."
      ],
      "metadata": {
        "id": "SwKa3Gr7oTmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.1. Predicción para Libros No Calificados**\n",
        "\n",
        "Para cada usuario, recomendamos los 10 libros con mayor probabilidad de gustarle."
      ],
      "metadata": {
        "id": "O6q0GbvsoYQq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OnwOtsM7obyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.2. Cálculo de la Diversidad Global**\n",
        "\n",
        "Calculamos la proporción de libros únicos recomendados respecto al total del catálogo."
      ],
      "metadata": {
        "id": "uAWt_dOXoevh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conjunto de libros recomendados\n",
        "recommended_books = set()\n",
        "\n",
        "for recs in recommendations.values():\n",
        "    recommended_books.update(recs)\n",
        "\n",
        "diversity = len(recommended_books) / len(all_isbns)\n",
        "print(f'Diversidad Global de Recomendaciones: {diversity:.4f}')\n"
      ],
      "metadata": {
        "id": "LsLRqr1ZoiKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Conclusiones\n",
        "\n",
        "En este laboratorio, hemos implementado un sistema de recomendación que combina técnicas avanzadas de procesamiento de lenguaje natural y aprendizaje automático:\n",
        "\n",
        "**BERT** nos permitió convertir títulos de libros en representaciones numéricas que capturan su significado semántico.\n",
        "\n",
        "**XGBoost** fue utilizado para entrenar un modelo de clasificación capaz de predecir si a un usuario le gustará un libro.\n",
        "\n",
        "Mediante **feature engineerering**, incorporamos información relevante sobre usuarios y libros, mejorando la capacidad predictiva del modelo.\n",
        "\n",
        "Las recomendaciones generadas ofrecen una diversidad global significativa, lo cual es beneficioso para mantener el interés del usuario y promover una exploración más amplia del catálogo.\n",
        "\n",
        "Este enfoque demuestra cómo la integración de diferentes técnicas puede conducir a soluciones efectivas en el desarrollo de sistemas de recomendación personalizados.\n",
        "\n"
      ],
      "metadata": {
        "id": "SyEci76dolpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Gracias por completar este laboratorio!"
      ],
      "metadata": {
        "id": "Pd7014q0o2E_"
      }
    }
  ]
}