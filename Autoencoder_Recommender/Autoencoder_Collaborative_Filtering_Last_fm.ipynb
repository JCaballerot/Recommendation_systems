{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JCaballerot/Recommender-Systems/blob/main/Autoencoder_Recommender/Autoencoder_Collaborative_Filtering_Last_fm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM6BxrBQ3mLH"
      },
      "source": [
        "\n",
        "<h1 align=center><font size = 5> ALS Collaborative Filtering - Last.fm </font></h1>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoAUCTsl3s9r"
      },
      "source": [
        "**Índice**\n",
        "\n",
        "- 1. Introducción\n",
        "- 2. Carga y Filtrado de Datos\n",
        "- 3. Creación del Modelo Autoencoder Convolucional\n",
        "- 4. Generación de Recomendaciones\n",
        "- 5. Validación\n",
        "- 6. Conclusiones\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fl2LA2h-Dj3"
      },
      "source": [
        "## 1. Introducción\n",
        "\n",
        "En este laboratorio, exploraremos el uso de autoencoders convolucionales para generar recomendaciones en el ámbito de las interacciones entre usuarios y artistas en Last.fm. Los autoencoders son modelos de aprendizaje no supervisado que aprenden a reconstruir su entrada pasando por una representación latente más compacta, capturando así las características esenciales de los datos. Al incorporar capas convolucionales, el modelo puede detectar patrones locales y estructuras complejas en los datos, lo que es especialmente útil en conjuntos de datos con relaciones no lineales y distribuciones heterogéneas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5xwdEsD-oGv"
      },
      "source": [
        "Instalamos las librerías necesarias.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMmEOEeW3kR_"
      },
      "outputs": [],
      "source": [
        "# Instalación de librerías necesarias\n",
        "!pip install tensorflow\n",
        "\n",
        "# Importación de librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lqygrXg-HfE"
      },
      "source": [
        "## 2. Carga y Filtrado de Datos\n",
        "\n",
        "Cargamos el dataset y aplicamos un filtro \"long tail\" para mejorar la calidad del análisis, manteniendo solo los artistas con al menos 50 escuchas. Este enfoque reduce el impacto de artistas menos populares y permite centrarse en recomendaciones más relevantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgpVbhOhEBZi"
      },
      "outputs": [],
      "source": [
        "# Descargar el dataset de Last.fm desde Kaggle\n",
        "!pip install kaggle\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()  # Sube tu archivo kaggle.json aquí\n",
        "\n",
        "# Configurar Kaggle API\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Descargar y descomprimir el dataset de Last.fm\n",
        "!kaggle datasets download -d japarra27/lastfm-dataset\n",
        "!unzip lastfm-dataset.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eMEuFfZEKr1"
      },
      "outputs": [],
      "source": [
        "# Cargar el dataset\n",
        "data = pd.read_parquet(\"lastfm_union.parquet\")[:10_000_000]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeuPHQpYS-3U"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoRQeQ20Ffoq"
      },
      "source": [
        "**Filtrado \"long tail\"**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HlijNVFENxg"
      },
      "outputs": [],
      "source": [
        "# Contar las escuchas por artista\n",
        "artist_listen_counts = data.groupby('artist_name').size().sort_values(ascending=False)\n",
        "\n",
        "# Visualizar distribución long tail\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(len(artist_listen_counts)), artist_listen_counts, color='lightblue')\n",
        "plt.title('Distribución del Número de Escuchas por Artista (Long Tail)')\n",
        "plt.xlabel('Artistas ordenados por popularidad')\n",
        "plt.ylabel('Número de escuchas')\n",
        "plt.ylim(1, 4000)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKx9JayQVC8H"
      },
      "outputs": [],
      "source": [
        "artist_listen_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xol-L6HtEQuo"
      },
      "outputs": [],
      "source": [
        "# Filtrar artistas con al menos 100 escuchas\n",
        "min_listens_per_artist = 100\n",
        "popular_artists = artist_listen_counts[artist_listen_counts >= min_listens_per_artist].index\n",
        "data_filtered = data[data['artist_name'].isin(popular_artists)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhL3kPdAVYnz"
      },
      "outputs": [],
      "source": [
        "# Filtrar usuarios con al menos 100 escuchas\n",
        "users_listen_counts = data_filtered.groupby('user_id').size().sort_values(ascending=False)\n",
        "users_listen_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnrN4PRsVjxG"
      },
      "outputs": [],
      "source": [
        "min_listens_per_user = 100\n",
        "popular_users = users_listen_counts[users_listen_counts >= min_listens_per_user].index\n",
        "data_filtered = data_filtered[data_filtered['user_id'].isin(popular_users)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERv1hfqaWmxI"
      },
      "outputs": [],
      "source": [
        "data_filtered.groupby('user_id').size().sort_values(ascending=False).tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW57MblaWzEr"
      },
      "outputs": [],
      "source": [
        "data_filtered.groupby('artist_name').size().sort_values(ascending=False).tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB5I3skLE8x1"
      },
      "source": [
        "## 3. Muestreo de datos\n",
        "\n",
        "Ahora que tenemos los datos filtrados, procederemos a entrenar un modelo de recomendación usando ALS con la biblioteca surprise. En surprise, podemos utilizar el algoritmo BaselineOnly con el método de estimación configurado como ALS.\n",
        "\n",
        "Primero, preparamos los datos en el formato que requiere surprise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADI6xPQQJ1KE"
      },
      "outputs": [],
      "source": [
        "# Crear el DataFrame con el recuento de escuchas\n",
        "user_artist_df = data_filtered.groupby(['user_id', 'artist_name']).size().reset_index(name='listens')\n",
        "user_artist_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFVJH3qCV-7-"
      },
      "outputs": [],
      "source": [
        "np.percentile(user_artist_df['listens'], 95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLaZqZPhJ77r"
      },
      "outputs": [],
      "source": [
        "trainset_scaled = user_artist_df\n",
        "trainset_scaled['listens'] = user_artist_df.listens/np.percentile(user_artist_df['listens'], 95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tZ9CAUZFpM_"
      },
      "source": [
        "**División del Conjunto de Datos**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IBLD7HIWTtC"
      },
      "outputs": [],
      "source": [
        "trainset_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zetFic39E_3l"
      },
      "outputs": [],
      "source": [
        "# Dividir en conjuntos de entrenamiento y prueba estratificando por usuario\n",
        "train_data, test_data = sk_train_test_split(trainset_scaled,\n",
        "    test_size = 0.2,\n",
        "    random_state = 42,\n",
        "    stratify = trainset_scaled['user_id']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JurrlwnQJM4U"
      },
      "source": [
        "## 4. Creación del Modelo Autoencoder Convolucional\n",
        "\n",
        "Implementaremos un autoencoder convolucional. Los autoencoders son modelos que aprenden a reconstruir su entrada pasando por una representación latente más pequeña, capturando así las características más importantes de los datos. Al utilizar convoluciones, el modelo puede capturar patrones locales en las interacciones usuario-artista."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Preparación de los Datos para el Autoencoder\n",
        "\n",
        "Primero, necesitamos preparar los datos en un formato adecuado para el autoencoder. Esto implica crear una matriz de interacción usuario-artista donde cada fila representa un usuario y cada columna representa un artista"
      ],
      "metadata": {
        "id": "9UQGS9Qkm9uu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5wQUT2QQdzn"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Copias de los conjuntos de datos para evitar advertencias\n",
        "train_data_enc = train_data.copy()\n",
        "test_data_enc = test_data.copy()\n",
        "\n",
        "# Inicializar los codificadores\n",
        "user_encoder = LabelEncoder()\n",
        "artist_encoder = LabelEncoder()\n",
        "\n",
        "# Ajustar y transformar los IDs de usuarios y artistas en train_data\n",
        "train_data_enc['user_id_enc'] = user_encoder.fit_transform(train_data_enc['user_id'])\n",
        "train_data_enc['artist_id_enc'] = artist_encoder.fit_transform(train_data_enc['artist_name'])\n",
        "\n",
        "# Transformar los IDs de usuarios y artistas en test_data utilizando los mismos codificadores\n",
        "test_data_enc['user_id_enc'] = user_encoder.transform(test_data_enc['user_id'])\n",
        "test_data_enc['artist_id_enc'] = artist_encoder.transform(test_data_enc['artist_name'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.2 Creación de la Matriz de Interacción\n",
        "\n",
        "Construimos la matriz de interacción utilizando los índices codificados."
      ],
      "metadata": {
        "id": "1jBtScVRoK1C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY1vth4YXqEp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Obtener el número total de usuarios y artistas en train_data\n",
        "num_users = train_data_enc['user_id_enc'].nunique()\n",
        "num_artists = train_data_enc['artist_id_enc'].nunique()\n",
        "\n",
        "# Crear la matriz de interacción vacía para train_data\n",
        "interaction_matrix = np.zeros((num_users, num_artists))\n",
        "\n",
        "# Rellenar la matriz con los valores de escuchas\n",
        "for row in train_data_enc.itertuples():\n",
        "    interaction_matrix[row.user_id_enc, row.artist_id_enc] = row.listens\n",
        "\n",
        "print(f\"Matriz de interacción de entrenamiento: {interaction_matrix.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.3 Normalización de los Datos\n",
        "\n",
        "Es recomendable normalizar los datos para que los valores estén entre 0 y 1, facilitando el entrenamiento del autoencoder."
      ],
      "metadata": {
        "id": "fo-p8AgyoRTy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "913lgO6cQrIa",
        "outputId": "c0fa663f-d311-4f5b-e5cf-94477972df00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimating biases using als...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.baseline_only.BaselineOnly at 0x7a2f07ebe860>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "# Normalizar los valores entre 0 y 1\n",
        "max_listens = interaction_matrix.max()\n",
        "interaction_matrix_norm = interaction_matrix / max_listens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Construcción del Autoencoder Convolucional\n"
      ],
      "metadata": {
        "id": "PmyObtsdoXMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora construiremos el modelo del autoencoder convolucional utilizando TensorFlow y Keras."
      ],
      "metadata": {
        "id": "7XqQLyvFofNM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl39q8hAJQrU",
        "outputId": "61a9f0b0-5971-4ba4-e64c-5770aa56a9b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R² en el conjunto de entrenamiento: 0.08873116760890731\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Añadir una dimensión adicional para las convoluciones\n",
        "interaction_matrix_reshaped = interaction_matrix_norm.reshape((interaction_matrix_norm.shape[0], interaction_matrix_norm.shape[1], 1))\n",
        "\n",
        "# Definir la entrada del modelo\n",
        "input_shape = (interaction_matrix_reshaped.shape[1], 1)\n",
        "input_layer = layers.Input(shape=input_shape)\n",
        "\n",
        "# Codificador\n",
        "x = layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
        "x = layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
        "x = layers.Conv1D(16, kernel_size=3, activation='relu', padding='same')(x)\n",
        "encoded = layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
        "\n",
        "# Decodificador\n",
        "x = layers.Conv1D(16, kernel_size=3, activation='relu', padding='same')(encoded)\n",
        "x = layers.UpSampling1D(size=2)(x)\n",
        "x = layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling1D(size=2)(x)\n",
        "\n",
        "# Capa de salida\n",
        "output_layer = layers.Conv1D(1, kernel_size=3, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "# Construir y compilar el modelo\n",
        "autoencoder = models.Model(inputs=input_layer, outputs=output_layer)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Resumen del modelo\n",
        "autoencoder.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Entrenamiento del Modelo\n"
      ],
      "metadata": {
        "id": "BRrlCrPkr2Sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos el autoencoder utilizando únicamente train_data."
      ],
      "metadata": {
        "id": "loKiz3oDr4xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo con el conjunto de entrenamiento\n",
        "history = autoencoder.fit(\n",
        "    interaction_matrix_reshaped, interaction_matrix_reshaped,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "1ZYKwwFNr627"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Generación de Recomendaciones\n",
        "\n"
      ],
      "metadata": {
        "id": "XohlYr4hvMru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Reconstrucción de la Matriz de Interacción\n"
      ],
      "metadata": {
        "id": "JlmLP5NwsBFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos las predicciones del modelo para todos los usuarios en train_data.\n",
        "\n"
      ],
      "metadata": {
        "id": "1AAO5x_XvP2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstruir la matriz de interacción\n",
        "reconstructed = autoencoder.predict(interaction_matrix_reshaped)\n",
        "\n",
        "# Remover la dimensión adicional\n",
        "reconstructed = reconstructed.reshape((interaction_matrix.shape[0], interaction_matrix.shape[1]))\n",
        "\n",
        "# Desnormalizar las predicciones\n",
        "reconstructed_denorm = reconstructed * max_listens\n"
      ],
      "metadata": {
        "id": "89US5R0vsGGx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPjnWvJbGT5f3DJ5a9+S482",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}